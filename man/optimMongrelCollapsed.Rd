% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{optimMongrelCollapsed}
\alias{optimMongrelCollapsed}
\title{Funtion to Optimize the Collapsed Mongrel Model}
\usage{
optimMongrelCollapsed(Y, upsilon, ThetaX, K, A, etainit, n_samples = 2000L,
  calcGradHess = TRUE, b1 = 0.9, b2 = 0.99, step_size = 0.003,
  epsilon = 1e-06, eps_f = 1e-08, eps_g = 1e-05, max_iter = 10000L,
  verbose = FALSE, verbose_rate = 10L, decomp_method = "eigen",
  eigvalthresh = 0, no_error = FALSE)
}
\arguments{
\item{Y}{D x N matrix of counts}

\item{upsilon}{(must be > D)}

\item{ThetaX}{D-1 x N matrix formed by Theta*X (Theta is Prior mean
for regression coefficients)}

\item{K}{D-1 x D-1 precision matrix (inverse of Xi)}

\item{A}{N x N precision matrix given by (I_N + X\emph{Gamma}X')^{-1}]}

\item{etainit}{D-1 x N matrix of initial guess for eta used for optimization}

\item{n_samples}{number of samples for Laplace Approximation (=0 very fast
as no inversion or decomposition of Hessian is required)}

\item{calcGradHess}{if n_samples=0 should Gradient and Hessian
still be calculated using closed form solutions?}

\item{b1}{(ADAM) 1st moment decay parameter (recomend 0.9) "aka momentum"}

\item{b2}{(ADAM) 2nd moment decay parameter (recommend 0.99 or 0.999)}

\item{step_size}{(ADAM) step size for descent (reocment 0.001-0.003)}

\item{epsilon}{(ADAM) parameter to avoid divide by zero}

\item{eps_f}{(ADAM) normalized function improvement stopping criteria}

\item{eps_g}{(ADAM) normalized gradient magnitude stopping criteria}

\item{max_iter}{(ADAM) maximum number of iterations before stopping}

\item{verbose}{(ADAM) if true will print stats for stopping criteria and
iteration number}

\item{verbose_rate}{(ADAM) rate to print verbose stats to screen}

\item{decomp_method}{decomposition of hessian for Laplace approximation
'eigen' (more stable, slower, default) or 'cholesky' (less stable, faster)}

\item{eigvalthresh}{threshold for negative eigenvalues in
decomposition of negative inverse hessian (should be <=0)}

\item{no_error}{if true will throw hessian warning rather than error if
not positive definite.}
}
\value{
List containing (all with respect to found optima)
\enumerate{
\item LogLik - Log Likelihood of collapsed model (up to proportionality constant)
\item Gradient - (if \code{calcGradHess}=true)
\item Hessian - (if \code{calcGradHess}=true)
\item Pars - Parameter value of eta at optima
\item Sampes - (D-1) x N x n_samples array containing posterior samples of eta
lbased on Laplace approximation (if n_samples>0)
}
}
\description{
See details for model. Should likely be followed by function
\code{\link{uncollapseMongrelCollapsed}}. Notation: \code{N} is number of samples,
\code{D} is number of multinomial categories, and \code{Q} is number
of covariates.
}
\details{
Notation: Let Z_j denote the J-th row of a matrix Z.
Model:
\deqn{Y_j ~ Multinomial(Pi_j)}
\deqn{Pi_j = Phi^{-1}(Eta_j)}
\deqn{Eta ~ T_{D-1, N}(upsilon, Theta*X, K^{-1}, A^{-1})}
Where A = (I_N + X * Gamma * X')^{-1}, K^{-1} = Xi is a (D-1)x(D-1) covariance
matrix, Gamma is a Q x Q covariance matrix, and Phi^{-1} is ALRInv_D
transform.

Gradient and Hessian calculations are fast as they are computed using closed
form solutions. That said, the Hessian matrix can be quite large
[N*(D-1) x N*(D-1)] and storage may be an issue.

Note: Warnings about large negative eigenvalues can either signal
that the optimizer did not reach an optima or (more commonly in my experience)
that the prior / degrees of freedom for the covariance (given by paramters
\code{upsilon} and \code{K}) were too specific and at odds with the observed data.
If you get this warning try the following.
\enumerate{
\item Try restarting the optimization using a different initial guess for eta
\item Try decreasing \code{step_size} and increasing \code{max_iter} parameters
in optimizer
\item Try relaxing prior assumptions regarding covariance matrix. (e.g., may want
to consider decrasing parameter \code{upsilon} closer to a minimum value of
D)
}

[N*(D-1) x N*(D-1)]: R:N*(D-1)%20x%20N*(D-1)%5C
}
\examples{
sim <- mongrel_sim()
attach(sim)

# Fit model for eta
fit <- optimMongrelCollapsed(Y, upsilon, Theta\%*\%X, K, A, 
                             random_mongrel_init(Y))  
}
\references{
S. Ruder (2016) \emph{An overview of gradient descent
optimization algorithms}. arXiv 1609.04747
}
\seealso{
\code{\link{uncollapseMongrelCollapsed}}
}
